# -*- coding: utf-8 -*-
"""Abalone_SHAP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wlWXUvCYv0rIseN4DoQ1NtdHjJOoKGVJ

#  Fetch data from UCIML
"""

pip install ucimlrepo

from ucimlrepo import fetch_ucirepo

# fetch dataset
abalone = fetch_ucirepo(id=1)

"""# Transform into dataframe

##df

### .data.features allows you to create a df with the features but not the target variable.

### .data.target gives you the df with target variable only.
"""

x = abalone.data.features
y = abalone.data.targets

#concat into a single dataframe
import pandas as pd
df = pd.concat([x, y], axis=1)

df.head()

df.shape

"""# Visualize the data

#Plot 1
"""

import matplotlib.pyplot as plt

plt.scatter(df['Whole_weight'], df['Rings'])

plt.xlabel('Whole Weight')    # Whole weight is the total weight of the abalone including the shell and the meat inside.
plt.ylabel('Rings')
plt.title('Scatter Plot of Whole Weight vs. Rings')
plt.show()

"""Generally, as the whole weight increases, the number of rings tends to increase as well. This suggests a positive correlation between these two variables. The amount of scatter in the plot tells us that the relationship is not perfectly linear. There might be other futures that are influencing the Number of rings in the abalone.

#Plot 2
"""

import seaborn as sns

plt.figure(figsize=(8, 6))
sns.boxplot(x='Sex', y='Rings', data=df)
plt.title('Box Plot of Rings by Sex')
plt.xlabel('Sex')
plt.ylabel('Rings')
plt.show()

"""The disribution of rings differs slightly among the 3 categories ("M" , "F", and  "I"). So, the sex of the abalone is also a feature that the target variable is dependent on.

#Plot 3
"""

df_corr = df.drop(columns=["Sex"])  # drop the categorical column
sns.heatmap(df_corr.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix Heatmap')
plt.show()

"""After droping the categorical column ("Sex"), The correlation of the variables are pretty positive. When you have a look at the target variable, every feature is having a positive impact towards the target. Since, Legth and Diameter are always proportional to each other, we got the highest coefficient value to them. And, Shell_weight has the most positive relation with Rings.

# Feature Engineering
"""

import pandas as pd
# Create dummy variables for the 'Sex' column
df = pd.get_dummies(df, columns=['Sex'], prefix=['Sex'],dtype=int)
df.head()

"""Now, we are ready to train our model.

# Modelling
"""

X = df.drop('Rings', axis=1)
y = df['Rings']

#from sklearn.model_selection import train_test_split
#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

import xgboost as xgb
model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)
model.fit(X, y)

y_pred = model.predict(X)

from sklearn.metrics import mean_squared_error, r2_score
# Evaluate the model
mse = mean_squared_error(y, y_pred)
r2 = r2_score(y, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R-squared: {r2}")

plt.figure()
plt.scatter(y, y_pred)
plt.plot([min(y), max(y)], [min(y), max(y)], linestyle='-', color='red', linewidth=2) # perfect prediction line
plt.xlabel("Actual Rings")
plt.ylabel("Predicted Rings")
plt.title("Actual vs. Predicted Rings")
plt.show()

"""The predictions are not too far from the red line, which indicatest the models is working fine.

# SHAP
"""

import shap
shap.initjs()

#shap values
xai = shap.TreeExplainer(model)
shap_values = xai(X)

shap_values.shape

"""## Waterfall plot"""

shap.plots.waterfall(shap_values[0])

"""The E[f(X)]= 9.933 is the average number of rings predicted by the model among all the 4177 abalones. f(x)=13.441 is predicted number of rings for the first observation(0). The left pane is the actual features value, for example as you can see Sex_M = 1 indicates this abalone is a male. Finally the shap values, shap value of shucked_weight is 1.09 which means the ffeature shucked_weight increased the predicted number of rings by 1.09 shap value. similary, the Sex_F decreased with 0.02 since its not a female."""

shap.plots.waterfall(shap_values[1],max_display=4)

"""Using the max_disply=4 , we can plot the top 4 features influecing the Rings either positively or negatively. This is for the second abalone(shap_values[1]).

## Force Plot
"""

shap.initjs()
shap.plots.force(shap_values[0])

"""Force plot is the condensed waterfall plot. We can see the base value is same. Force plot alos helps us to identify the individual prediction.

## Stacked Force Plot
"""

shap.initjs()
shap.plots.force(shap_values[0:10])

"""# Absolute Mean SHAP

This plot gives us the absolute mean value for that feature. The larger the bar the more the feature influence/impact either positive or negative way. **ABM SHAP plot is used as metric for feature importnace.**
"""

shap.plots.bar(shap_values)

"""## Beeswarm Plot

The single most used plot. It visualizes all of the shap values. The colour of the point represents the feature value but not the shap value, blue for lower and red for higher. You can notice the oreder is based on absolute mean. The red colour in the left direction in shucked weight indicates that the higher the feature value the lower the shap value and the influence.
"""

shap.plots.beeswarm(shap_values)

"""# Dependence Plot

Scatterplot is shap values for a single feature. These are particularly used to know if there is any non-linearity relationship with target variable. Looking at the beeswarm plot we think the shell_weight is linear but when you see the scatter plot its is not that linear.
"""

shap.plots.scatter(shap_values[:,"Shell_weight"])

shap.plots.scatter(shap_values[:,"Shucked_weight"])

"""As we concluded in the beeswarm plot, that they are inversely proportional, just to confirm we plotted an another scatter plot.

You can add other feature as color, notice that the shap value is large when both the shell_weight and shucked_weigth is larger.
"""

shap.plots.scatter(shap_values[:,"Shell_weight"],
                   color=shap_values[:,"Shucked_weight"])

